 # AI Hiring Assistant at FinTech Corp
**What’s happening?** FinTech Corp uses an AI to scan résumés and rank job applicants based on skills, experience, and predicted performance. The system was trained on past hiring data.

**What could go wrong?** Here’s where things get murky. If historical data contains bias—like favoring applicants from certain universities or demographics—the AI may learn and perpetuate those preferences. That means equally qualified applicants from underrepresented backgrounds might get unfairly ranked lower.

**Responsible fix:** Retrain the AI using fairness-aware techniques like reweighting biased samples or introducing synthetic diversity. Also, include explainable AI features that show why each résumé was ranked the way it was.

**Blog-style reflection:**

Imagine having your whole career judged by a robot trained on someone else’s biases. Yikes. FinTech’s AI thought it was choosing the best—but really, it was repeating history. A little fairness-aware retraining can turn this résumé scanner from gatekeeper to game-changer.
